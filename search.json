[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "This page will showcase academic and personal projects. Currently under development as part of the migration from Next.js to Quarto."
  },
  {
    "objectID": "projects.html#academic-projects",
    "href": "projects.html#academic-projects",
    "title": "Projects",
    "section": "Academic Projects",
    "text": "Academic Projects\n\nPh.D. Dissertation: Research on Investor-State Dispute Settlement (ISDS)"
  },
  {
    "objectID": "projects.html#data-science-programming",
    "href": "projects.html#data-science-programming",
    "title": "Projects",
    "section": "Data Science & Programming",
    "text": "Data Science & Programming\n\nR Automation: Scripts for email automation and workflow optimization\nAcademic Writing: Tools for managing citations, references, and academic workflows\nBlog Migration: Converting from Next.js/MDX to Quarto for better academic publishing"
  },
  {
    "objectID": "projects.html#open-source-contributions",
    "href": "projects.html#open-source-contributions",
    "title": "Projects",
    "section": "Open Source Contributions",
    "text": "Open Source Contributions\nFuture contributions to the R and Quarto ecosystems as I develop my technical skills alongside my academic work.\n\nThis projects page is being developed. Check back for updates on current research and coding projects."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am currently a Ph.D. student at the University of Nevada, Las Vegas, where I study international relations in the Department of Political Science. My research focuses on international courts’ role in the world economy, specifically as they relate to foreign direct investment and multinational corporations. My main academic interests are in:\n\nhow investor-state dispute settlement (ISDS) arbitrations proceed from the (alleged) treaty violation to the concluding decision;\nhow the design and proliferation of investment agreements impact firms’ behavior with regard to ISDS.\n\nTogether, these research interests form a research agenda that seeks to deepen our understanding of how institutional design shapes behavior within the ISDS regime—from the strategic calculations of states and multinational corporations facing investment decisions to the procedural dynamics that influence arbitral outcomes.\nThis research requires analyzing large datasets of arbitration cases, legal texts, and corporate behavior, which has cultivated in me both advanced quantitative capabilities and a genuine passion for data science and programming. I began my learning journey with R as my first language, but I’ve come to transition more into Python due to its superior capabilities for the machine learning and natural language processing tasks essential to my research. These computational approaches allow me to analyze thousands of ISDS cases and legal documents at scale, uncovering patterns invisible to traditional qualitative methods.\nMy current technical focus includes:\n\nusing large language models for natural language inference tasks in legal documents and public political discourse;\ntraining Word2Vec and Doc2Vec models for document classification;\nemploying computer vision models (like Detectron2) for image segmentation tasks in legal documents.\n\nOriginally from Mexicali, I’ve called Las Vegas home for most of my life. When I’m not working on research, I enjoy reading philosophy and literature, and spending time with friends and family."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "All Posts",
    "section": "",
    "text": "Browse all blog posts organized by date and category. Use the filters above to find content on specific topics.\n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nMigrating to Quarto\n\n\n\nR\n\nQuarto\n\nCode\n\n\n\nI migrated my website to Quarto!\n\n\n\n\n\nAug 12, 2025\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nWriting on Watches\n\n\n\nHorology\n\nWriting\n\n\n\nSetting out to write more and explore a hobby.\n\n\n\n\n\nMar 6, 2025\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Probability Axioms\n\n\n\nNotes\n\nProbability\n\n\n\n\n\n\n\n\n\nFeb 3, 2025\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Convert a Quarto Reveal.js Deck to PDF\n\n\n\nQuarto\n\nBash\n\nPDF\n\n\n\nA simple guide to converting a Quarto Reveal.js deck to a PDF\n\n\n\n\n\nJan 27, 2025\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nThe Spring 2025 Semester Kicks Off\n\n\n\nPhD\n\n\n\nThe Spring semester has officially begun and I’ve already had a bunch of students come by.\n\n\n\n\n\nJan 25, 2025\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nI Just Finished My Prospectus\n\n\n\nPhD\n\n\n\nI finished my prospectus and I’m looking forward to my defense.\n\n\n\n\n\nJan 18, 2025\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nLaTeX Issue Fix: No , , or commands\n\n\n\nLaTeX\n\nbibliography\n\ncitations\n\nbiblatex\n\ntroubleshooting\n\n\n\nA potential fix for an annoying LaTeX error\n\n\n\n\n\nJan 3, 2025\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Convert a LaTeX Document to a DOCX File\n\n\n\nLaTeX\n\nWord\n\nCode\n\nPandoc\n\n\n\nA quick guide to converting a LaTeX document to a DOCX file.\n\n\n\n\n\nDec 16, 2024\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Make a Bingo Card in R\n\n\n\nR\n\nBingo\n\nCode\n\n\n\nA quick guide to making a bingo card in R.\n\n\n\n\n\nNov 3, 2024\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nNotes on Bayes’ Formula\n\n\n\nNotes\n\nBayes\n\nFormula\n\nData Science\n\n\n\nQuick notes on Bayes’ Formula and how to Calculate it.\n\n\n\n\n\nSep 21, 2024\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Make an R Project Template\n\n\n\nR\n\nRStudio\n\nCode\n\nProjects\n\n\n\nA quick guide to setting up an R project template in RStudio.\n\n\n\n\n\nMay 28, 2024\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nGreek Alphabet in Markdown\n\n\n\nMarkdown\n\nRMarkdown\n\nGreek Alphabet\n\n\n\nQuick notes regarding Greek alphabet notation in markdown.\n\n\n\n\n\nMay 19, 2024\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nMarkdown Notation\n\n\n\nMarkdown\n\nRMarkdown\n\nMath Notation\n\n\n\nQuick notes regarding mathematical notation in markdown.\n\n\n\n\n\nMay 19, 2024\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nRecurring Emails Using R\n\n\n\nR\n\nemail\n\nautomation\n\n\n\nSimple code for using R to send recurring emails\n\n\n\n\n\nJan 11, 2024\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Migrate All R Packages to a New Computer\n\n\n\nR\n\nRStudio\n\nCode\n\nPackages\n\n\n\nEasy way to migrate user-installed packages.\n\n\n\n\n\nOct 21, 2023\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nZero-Inflated Models\n\n\n\nNotes\n\nModel\n\nData Science\n\nZero-Inflation\n\nCount Data\n\n\n\nQuick notes for Zero-inflated models.\n\n\n\n\n\nOct 18, 2023\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nNegative Binomial Model\n\n\n\nNotes\n\nModel\n\nData Science\n\nCount\n\n\n\nQuick notes for the negative binomial model.\n\n\n\n\n\nOct 15, 2023\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Model\n\n\n\nNotes\n\nModel\n\nData Science\n\nPoisson\n\n\n\nQuick notes for Poisson Model.\n\n\n\n\n\nOct 8, 2023\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\nNotes\n\nModel\n\nData Science\n\nMultinomial\n\nLogit\n\n\n\nQuick notes for Multinomial Logit Model\n\n\n\n\n\nOct 2, 2023\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nStereotype Logit Model\n\n\n\nNotes\n\nStereotyped Logit\n\nModel\n\nData Science\n\n\n\nQuick notes for Stereotyped Logit Models.\n\n\n\n\n\nSep 24, 2023\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nAdjacent Categories Models\n\n\n\nNotes\n\nAdjacent Categories\n\nModel\n\nData Science\n\n\n\nQuick notes for Adjacent Categories Models.\n\n\n\n\n\nSep 24, 2023\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nContinuation Ratio Model\n\n\n\nNotes\n\nContinuation Ratio\n\nModel\n\nData Science\n\n\n\nQuick notes for continuation ratio model.\n\n\n\n\n\nSep 23, 2023\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Ordinal Logistic Regression Models and Partial Proportional Odds Models\n\n\n\nNotes\n\nProportional Odds\n\nModel\n\nData Science\n\nOrdered Logit\n\n\n\nQuick notes for PPO and GOL models.\n\n\n\n\n\nSep 15, 2023\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nProportional Odds Models\n\n\n\nNotes\n\nOrdered Logit\n\nModel\n\nData Science\n\nProportional Odds\n\n\n\nQuick notes for the proportional odds model.\n\n\n\n\n\nSep 13, 2023\n\n\nKevin Navarrete-Parra\n\n\n\n\n\n\n\n\n\n\n\n\nLogit Notes\n\n\n\nNotes\n\nLogit\n\nModel\n\nData Science\n\n\n\nQuick notes for logit model.\n\n\n\n\n\nSep 6, 2023\n\n\nKevin Navarrete-Parra\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/quarto-deck-to-pdf.html",
    "href": "posts/quarto-deck-to-pdf.html",
    "title": "How to Convert a Quarto Reveal.js Deck to PDF",
    "section": "",
    "text": "My favorite way to make slide decks is to use Quarto, which is a Markdown-based document processor that allows you to create documents in a variety of formats using the simple Markdown typesetting language. One feature that I utilize more often than any other is its ability to create slide decks using the Reveal.js framework in a simple and straightforward way. Usually, I’ll publish these decks to Quarto Pub, which freely and simply hosts my slides online, and which makes it possible to easily share a link to the presentation for others to view more conveniently.\nDespite the convenience of Quarto for my deck-making needs, I’ve found that I’m occasionally dissatisfied whenever I need to export them to PDF. I know that I can open the deck in the browser, press e to enable export mode, and print the document using my browser’s print dialog, but I’ve found that the resulting PDFs don’t often look how I’d like. For instance, recently the PDF I generated in this way had too much of a margin, which ate up some footnotes,1 and the deck saved as entirely white with black text, despite the Reveal.js theme I was using.\nAfter some quick searching, I found a neat alternative that I thought I’d share here so I remember it in the future. If you download the decktape npm package, you can easily convert your Quarto Reveal.js deck to a PDF using the following command:\nIn my case, I provided a URL to the Quarto Pub link where my deck was hosted, and the command generated a PDF that looked exactly like the deck I had created in Quarto. I was very pleased with the result, and I’ll definitely be using this method in the future."
  },
  {
    "objectID": "posts/quarto-deck-to-pdf.html#footnotes",
    "href": "posts/quarto-deck-to-pdf.html#footnotes",
    "title": "How to Convert a Quarto Reveal.js Deck to PDF",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI even tried messing with the margins in the print dialog, but it didn’t help.↩︎"
  },
  {
    "objectID": "posts/r-packages-install.html",
    "href": "posts/r-packages-install.html",
    "title": "How to Migrate All R Packages to a New Computer",
    "section": "",
    "text": "If you have any questions or comments regarding the code below, feel free to reach out to me. I am always happy to talk R with anyone!\nIt can be a bit of a hassle to install R and RStudio on a new computer, especially when you have hundreds of user-downloaded packages you’d like to migrate to the new computer. You could copy the folder that has all your packages installed, but there are bound to be issues that arise from such a brute-force solution–especially if you’re switching from one operating system to another. Below is some simple code that helped me switch from using R in Windows to Mac. Before I proceed, though, I would like to thank Andrew Z. from R-Bloggers for the helpful post that inspired this one.\nI’ve split the code into three chunks. The first generates a list of all the user-installed packages in your original R installation. The second takes that list and makes it into a downloadable vector. Finally, the third chunk takes the .txt file and uses it to download all the R packages on your new machine.\nImportantly, the first two chunks should be run on the original machine and the third should be run on the new one.\n\nGenerating the List\nThe code below is what generates the list of user-installed packages in your original computer’s R installation.\npacks &lt;- as.vector(installed.packages()[,1])\n\nprint(packs, row.names = FALSE)\nAs you can see from the code, you start by generating a vector with the installed.packages function. However, it is important to subset the data because the function will return a lot of unnecessary information–along with the base packages. The, we print all the packages to see if we got what we wanted.\n\n\nSaving the vector\nImportantly, we want to make sure that all the packages are surrounded by quotation marks for when we input the vector in the install.package function. Moreover, when we write the vector as a txt file, we want to ensure that all the packages have comma delimiters so that they work properly in the coming code.\n\npacks1 &lt;- paste0('\"', packs, '\"')\n\nwriteLines(paste(packs1, collapse = \",\"), \"package_list.txt\")\n\n\nDownloading to the New Computer\nFinally, we get to the bit of code that we’ll run on the new computer! As we can see below, the first step is going to be bringing in the .txt file. You can do so with the read.delim function, which will generate an object with all the desired packages. Next, we take the generated object and convert it into a vector with the desired properties. And finally, we install the vector of packages to the new computer.\nsetwd(\"/Users/kevin/Downloads\")\n\ndata1 &lt;- read.delim('package_list.txt', header = FALSE, sep = \",\")\n\npack_vec &lt;- as.vector(unlist(t(as.matrix(data1[,-1]))))\n\ninstall.packages(pack_vec)"
  },
  {
    "objectID": "posts/Notes/ppo-notes.html",
    "href": "posts/Notes/ppo-notes.html",
    "title": "Proportional Odds Models",
    "section": "",
    "text": "I am writing quick and easy R guides for my didactic purposes and to provide useful starting places for my peers in grad school. If you see that I have made a mistake or would like to suggest some way to make the post better or more accurate, please feel free to [email][1] me. I am always happy to learn from others’ experiences!\n\nProportional odds model\nThe proportional odds model equation is\n logit[\\pi_j(x)] = ln(\\frac{\\pi_j(x)}{1-\\pi_j(x)}) = \\alpha_j + (-\\beta_1X_1 - \\beta_2X_2 - ... - \\beta_pX_p) \nwhere \\pi_j(x) = P(Y \\le j| x_1,x_2, ..., x_p), which is the probability (P) of being at or below a given category (j) given a set of predictors. The \\beta values are the logit coefficients and \\alpha_j is the number of cut points.\nTo run a proportional odds model, you can run the clm function from the ordinal package or the VGLM function from VGAM package.\n\n\nEstimating log odds\nEstimating the ln(odds) of being at or below the jth category means the model is rewritten as\n logit[P(Y \\le j| x_1,x_2, ..., x_p)] = (\\frac{P(Y \\le j| x_1,x_2, ..., x_p)}{P(Y &gt; j| x_1,x_2, ..., x_p)}) = \\alpha_j + (-\\beta_1X_1 - \\beta_2X_2 - ... - \\beta_pX_p) \nThe only new thing in the equation above is the large fraction in parenthesis, and that just explains what’s going on to the left and right.\nThe proportional odds model effectively acts like several logistic regression models estimated simultaneously. Their outcomes are dichotomized from the ordinal outcome variable to compare the probabilities of being below or above a given category (Y\\le j & Y &gt; j). Although each model has different intercepts, all models’ estimated logit coefficients are constrained to be equal. Hence the proportional odds part of the model because the regression lines are parallel.\nLike with logit models, the odds of a given outcome are\n Odds(Y\\le j) = \\frac{P(Y\\le j)}{1-P(Y\\le j)} \nThe cumulative probability is the probability of being less than or equal to P(Y \\le j) since it equals the sum of all categories’ probabilities\n P(Y \\le j) = P(Y=1) + P(Y=2) + ... + P(Y=j) \\ \\ \\ \\ When j = 1, 2, ..., j \nWhen comparing categories in a proportional odds model, you compare the given category to the remaining categories.\n\n\nOdds ratios in proportional odds models\nCalculating the odds ratio for the proportional odds model is similar to the logit model, but requires an additional step. Whereas the explanation for categories above the jth level is calculated as exp(\\beta), the odds for categories below are calculated as exp(- \\beta). In other words, you exponentiate the inverse of the given coefficient.\n\n\nProportional odds assumption\nThe model’s fundamental assumption is that each independent variable has the same effects across all categories of the dependent variable.\nIn order to test the proportional odds assumption, we employ a likelihood ratio test. In R, we can run this test using the nominal_test function in the ordinal package or the lrtest function from the VGAM package.\nOther tests for the model include pseudo-R^2, deviance, likelihood ratio test, AIC, and BIC, like with the logit model. For the pseudo-R^2 values, you can employ the nagelkerke function from the rcompanion package.\nYou can generate an odds ratio matrix for the proportional odds model by running\ncbind(exp(coef(model)), exp(confint(model)))\nTo test the log-likelihood for a given proportional odds model, you can run\nanova(model1, model2)\nUsing the ggpredict function from the ggeffects package, we can generate margin tables for the given predictor variables."
  },
  {
    "objectID": "posts/Notes/continuation-ratio-model.html",
    "href": "posts/Notes/continuation-ratio-model.html",
    "title": "Continuation Ratio Model",
    "section": "",
    "text": "I am writing quick and easy R guides for my didactic purposes and to provide useful starting places for my peers in grad school. If you see that I have made a mistake or would like to suggest some way to make the post better or more accurate, please feel free to email me. I am always happy to learn from others’ experiences!\n\nTable of contents\n\nModel Formula\nConditional Probabilities and Odds Ratios\nRunning it in R\nDiagnostic Statistics\n\n\n\nModel Formula \nBefore we begin looking at the model’s equation, it’s important to begin by distinguishing this from the proportional odds and generalized ordinal logit models. The latter two models estimate the probability of being at or above a given category (or at or below a given category). Instead, the continuation ratio model estimates the odds of being in a given category versus being above that category. In other words, this model is more suited to estimating the odds of attaining a given category, assuming that the response variable represents successive, ordered stages.\nThe model is estimated as\n ln \\left(\\frac{P(Y &gt; j | x_1, x_2, ..., x_p)}{P(Y = j | x_1, x_2, ..., x_p)}\\right) = \\alpha_j + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p \nwhere P(Y &gt; j | x_1, x_2, ..., x_p) is the conditional probability of being above j, given one already is in that given category. As with the PO and GOL models, j = 1,2, ..., J - 1. \\alpha_j represents the cutoff points and the \\beta values represent the logit coefficients.\nSimilarly, the model can estimate the odds of being in a given category relative to being above that category.\n ln \\left(\\frac{P(Y = j | x_1, x_2, ..., x_p)}{P(Y &gt; j | x_1, x_2, ..., x_p)}\\right) = \\alpha_j + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p \nwhere all else about the model is equal except for the switched conditional probabilities on the left side of the equation.\nImportantly, the continuation ratio model is similar to the proportional odds model in that it assumes the logit coefficients are parallel across the ordinal categories.\n\n\nConditional Probabilities and Odds Ratios \nConditional probabilities in the CR model act a lot like cumulative probabilities in the PO model, except that they calculate the odds of being in a category, given you’re at or above that category. The conditional odds are calculated as\n Odds = \\frac{P(Y = j)}{P(Y&gt;j)} \nAnd like with other odds ratios, the conditional probabilities represent the change in odds given a one-unit increase in the predictor.\n\n\nRunning it in R \nYou can run the continuation ratio model in R using the vglm function from the VGAM package. The syntax is similar to the proportional odds and generalized ordinal logit models, except you use either the sratio or cratio family. The former estimates the stopping ratio, which is P(Y=j)/P(Y&gt;j), and the latter is the continuation ratio, which is P(Y&gt;j)/P(Y=j). Simply put, the stopping ratio estimates the conditional probability of being at or before value j while the continuation ratio estimates the conditional probability of an event happening after j, given j has not already happened.\nWhen running the CR model, make sure to stipulate parallel = TRUE in the family to ensure the model is abiding by the proportional odds assumption. You can run this model with parallel = FALSE to specify a model that follows the proportional odds assumption. Additionally, make sure to add reverse = FALSE to the family parameters to make sure there is no interpretive confusion.\nAfter all that is specified, interpreting the model is very similar to the proportional odds model. Make sure to run\n\nmodel.or &lt;- cbind(exp(coef(model)), exp(confint(model)))\nprint(model.or)\nto get the odds ratio for the given model. Interpreting the odds ratios works just like it does for the proportional odds and generalized ordinal logit models.\n\n\nDiagnostic Statistics \nThe model fit statistics for the CR model are the same as those used by the PO and GOL models. You can run a likelihood ratio test to test the model’s overall fit. Make sure to create a null version of your model for this test.\n\nmodel.null &lt;- vglm(dv ~ 1, sratio(parallel = TRUE, reverse = FALSE))\nsummary(model.null)\n\nlrtest(model.null, model)\nThe null hypothesis is that the specified variables do not contribute to the model and the alternate hypothesis is that the predictors contribute to a better-fitting model relative to the null model. Therefore, a significant result for the fitted model will allow us to reject the null hypothesis, indicating that the predictors contribute to a better-fitting model.\nAdditionally, you can use Pseudo R^2, which will be the same as for the logit, PO, and GOL models. See the logit notes for a deeper dive into the Pseudo R^2 diagnostics. Recall that these values act like the R^2 values for OLS models, but they’re are not entirely interchangeable.\nConsult the logit notes for a deeper dive into the AIC and BIC values as well. Recall that lower values of AIC and BIC are better than higher ones. Additionally, the BIC is slightly preferable to the AIC because the former rewards more parsimonious models.\nFinally, you can use ggpredict from ggeffects to create marginal effects plots for the model\n\nprd.m &lt;- ggpredict(model, terms = \"iv[x, y, z]\")\nmarg.m &lt;- plot(prd.m)\nwhere “iv[x, y, z]” indicates the given independent variable for which you’ll be plotting the marginal effects plot at three cutoff points."
  },
  {
    "objectID": "posts/Notes/generalized-ppo.html",
    "href": "posts/Notes/generalized-ppo.html",
    "title": "Generalized Ordinal Logistic Regression Models and Partial Proportional Odds Models",
    "section": "",
    "text": "I am writing quick and easy R guides for my didactic purposes and to provide useful starting places for my peers in grad school. If you see that I have made a mistake or would like to suggest some way to make the post better or more accurate, please feel free to [email][1] me. I am always happy to learn from others’ experiences!"
  },
  {
    "objectID": "posts/Notes/generalized-ppo.html#odds-and-odds-ratios",
    "href": "posts/Notes/generalized-ppo.html#odds-and-odds-ratios",
    "title": "Generalized Ordinal Logistic Regression Models and Partial Proportional Odds Models",
    "section": "Odds and Odds Ratios",
    "text": "Odds and Odds Ratios\nAs with the proportional odds model and logit model, you must calculate the odds ratio for your coefficients. Therefore, the odds of being at or below the jth category are expressed as\n Odds(Y \\le j) = \\frac{P(Y \\le j)}{P(Y &gt; j)} \nNote that the odds equation above works for the PPO model in which you estimate the odds of being at or below the jth category. For the odds of being at or above the jth category would be expressed as\n Odds(Y \\ge j) = \\frac{P(Y \\ge j)}{P(Y &lt; j)} \nFor a more thorough discussion of odds ratios and their interpretation, see the notes for logit and proportional odds models ."
  },
  {
    "objectID": "posts/Notes/generalized-ppo.html#goodness-of-fit",
    "href": "posts/Notes/generalized-ppo.html#goodness-of-fit",
    "title": "Generalized Ordinal Logistic Regression Models and Partial Proportional Odds Models",
    "section": "Goodness of Fit",
    "text": "Goodness of Fit\nThe goodness of fit statistics for the PPO model are the same as those for the proportional odds and logit models. See the notes for those models for explanations of deviance, likelihood ratio test, and pseudo-R^2 measures."
  },
  {
    "objectID": "posts/Notes/markdown-notation.html",
    "href": "posts/Notes/markdown-notation.html",
    "title": "Markdown Notation",
    "section": "",
    "text": "The following table is a recreation of Table 1.2 found in A Mathematics Course for Political & Social Research by Will H. Moore and David A. Siegel, with a few additions and changes1. This table is a quick reference for mathematical notation in Markdown and RMarkdown, and it can also be useful for formatting math equations in LaTeX. Additionally, it is worth noting that any of these inline code snippets can also be expressed as code blocks in markdown and Rmarkdown by wrapping them in double dollar signs, e.g., $$a + b$$."
  },
  {
    "objectID": "posts/Notes/markdown-notation.html#footnotes",
    "href": "posts/Notes/markdown-notation.html#footnotes",
    "title": "Markdown Notation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI’ve added some additional symbols and changed the formatting slightly for my purposes.↩︎"
  },
  {
    "objectID": "posts/Notes/logit-notes.html",
    "href": "posts/Notes/logit-notes.html",
    "title": "Logit Notes",
    "section": "",
    "text": "I am writing quick and easy R guides for my didactic purposes and to provide useful starting places for my peers in grad school. If you see that I have made a mistake or would like to suggest some way to make the post better or more accurate, please feel free to [email][1] me. I am always happy to learn from others’ experiences!"
  },
  {
    "objectID": "posts/Notes/logit-notes.html#using-deviance-to-compare-nested-models",
    "href": "posts/Notes/logit-notes.html#using-deviance-to-compare-nested-models",
    "title": "Logit Notes",
    "section": "Using Deviance to Compare Nested Models",
    "text": "Using Deviance to Compare Nested Models\nA nested model is a model whose parameters are a subset of another model’s parameters. The model with fewer parameters is the reduced model and the one with the full set of parameters is the full model. Recall that parameters are the number of independent variables plus the intercept. Suppose that you have two logit models, then. The first has x_1 + x_2 + x_3 as the independent variables and the second only has x_1 + x_2 as independent variables. The first would be the full model and the second would be the reduced model. You compare these two models by finding the difference between their 2LL values (G = D_{Reduced}-D_{Full})."
  },
  {
    "objectID": "posts/Notes/nagative-binomial.html",
    "href": "posts/Notes/nagative-binomial.html",
    "title": "Negative Binomial Model",
    "section": "",
    "text": "I am writing quick and easy R guides for my didactic purposes and to provide useful starting places for my peers in grad school. If you see that I have made a mistake or would like to suggest some way to make the post better or more accurate, please feel free to email me. I am always happy to learn from others’ experiences!\n\nTable of contents\n\nNegative Binomial Model\n\nModel Equation\nNegative Binomial Distribution\n\nIncidence Rate Ratios\nRunning it in R\nDiagnostic Statistics\n\n\n\nNegative Binomial Model \n\n\nModel Equation \nThis model is similar to the Poisson Model in that it is suited for count dependent variables. However, the Poisson model assumes that the response variable’s mean and variance are equal–a condition that most real data commonly violate. If your response variable suffers from overdispersion (i.e., the dependent variable’s variance is greater than its mean), then the negative binomial model is a good alternative. Whereas the Poisson model assumes the variable follows a Poisson distribution, the negative binomial model works off of the negative binomial distribution, which relaxes the dispersion assumption. In order to do so, the Variance (Y) is made into a function of the mean \\mu and a dispersion parameter \\alpha. Therefore, $Variance (Y) = (1 + ). When \\alpha = 0, the variance is equal to the mean, making the negative binomial identical to the Poisson, under these circumstances.\nThe negative binomial model is similar to the Poisson model in that it can be modeled as\n ln(\\mu) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon \nwhere mu is the mean, \\beta_0 is the intercept, the remaining \\beta values are coefficients, and \\epsilon is an error term. The equation’s left side contains the log link function. We include an error term in this model to reflect the overdispersion. Notably, there are two ways of expressing the response variable’s variance: as a linear equation Variance (Y) = \\mu(1 + \\mu) or a quadratic equation Variance (Y) = \\mu(1 + \\alpha \\mu) = \\mu + \\alpha \\mu^2. The former is often called the NB1 model and the latter the NB2 model, and the latter is the one that most people use.\nWhen trying to predict the response variable’s mean, we exponentiate both sides of the negative binomial model.\n \\mu = exp(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p) \nNotice that the equation above does not include the \\epsilon value. That is because exp(\\epsilon) is assumed to be equal to 1 in this model as with the Poisson model, so it would be redundant to include it.\nAdditionally, as with the Poisson model, we can incorporate a temporal element if we want to represent a count of occurrences in a given timeframe t. If we do define an incidence rate, the model can be re-expressed as\n ln(\\mu / t) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon \nwhere t is the time period and ln(\\mu / t) represents the log of the incidence rate. We can also rewrite the equation above as\n ln(\\mu) = ln(t) + \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon \nwhere ln(t) is the offset in the model.\n\n\nNegative Binomial Distribution \nAs we saw above, the negative binomial model handles overdispersion in count data better than the Poisson model. That is because the negative binomial probability distribution handles greater variance better than the Poisson distribution. In this probability distribution, we count all the independent Bernoulli trials before a given number of achieved successes. We can express the negative binomial probability distribution as\n P(Y = y) = \\binom{y + k - 1}{y} p^k (1-p)^y \nwhere \\binom{y = k - 1}{y} is the negative binomial coefficient, y is the number of trials before an achieved success, k is the number of successes in y + k trials, and p is the given trial’s success probability.\n\n\nIncidence Rate Ratios \nThe negative binomial model employs incidence rate ratios like the Poisson model, estimating the response variable’s log incidence rate. In order to get this value, then, we exponentiation both sides of the model.\n\n\nRunning it in R \nWhen you want to fit a negative binomial model in R, you can use the glm.nb function from the MASS package. The code would look like\n\nnb.mod &lt;- glm.nb(dv ~ iv1 + iv2 + ivp, data = data)\nsummary(nb.mod)\n\nYou can also run the same model using the vglm function from VGAM using the following code:\n\nnb.vglm &lt;- vglm(dv ~ iv1 + iv2 + ivp, family = neginomial, data = data)\nsummary(nb.vglm)\n\nUsing the glm.nb might be somewhat easier, unless you are specifying other models in the VGAM package.\n\n\nDiagnostic Statistics \nThe diagnostic statistics for this model are mostly the same as those used for the Poisson model and others I’ve covered here."
  },
  {
    "objectID": "posts/Notes/poisson-notes.html",
    "href": "posts/Notes/poisson-notes.html",
    "title": "Poisson Model",
    "section": "",
    "text": "I am writing quick and easy R guides for my didactic purposes and to provide useful starting places for my peers in grad school. If you see that I have made a mistake or would like to suggest some way to make the post better or more accurate, please feel free to email me. I am always happy to learn from others’ experiences!\n\nTable of contents\n\nModel Formula\nIncidence Ratios\nRunning it in R\nDiagnostic Statistics\n\n\n\nModel Formula \nPoisson models are useful for running regressions on count response variables (i.e., nonnegative integers that follow a Poisson distribution). You can represent this model as\n ln(\\mu) = \\alpha + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p \nwhere \\mu represents the average or expected average of observed events in the dependent variable, \\alpha is the intercept, and the \\beta values are the coefficients. As you can see, ln(\\mu) uses the log link.\nImportantly, the Poisson model assumes that the mean of the count response variable is equal to the variable’s variance, such that E(Y) = Variance (Y) = \\mu. We get the predicted mean for the count response by exponentiating both sides of the equation.\n \\mu = exp( \\alpha + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p ) \nWe can also specify the model so that the response variable represents a count value within a given set of times, which is called the incidence rate. This model can be specified as\n ln(\\mu /t) = \\alpha + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p \nwhere t is a period of time and \\mu /t indicates the incidence rate. You can also represent this equation as\n ln(\\mu) = ln(t) + \\alpha + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p \nwhere ln(t) is the offset in the model equation.\nThe Poisson model assumes the response variable follows the Poisson probability distribution, which can be expressed as\n P(y) = \\frac{e^{-u} u^y}{y!} \nwhere y is the count value of the response variable, \\mu is the expected or average of events, and y! is a factorial of the response. Note that \\mu is often represented as \\lambda as well.\nIn the Poisson distribution, the count variable’s mean is equal to the variable’s variance.\n \\mu = E(y) = Variance(y) \nThe log-likelihood function for the Poisson distribution can be expressed as\n l(\\mu;y) = \\sum^{n}_{i=1}{y_1 ln\\mu_i - \\mu_i - ln(y_i!)} \nwhere l(\\mu;y) is the log likelihood function of \\mu given the values of the count variable.\n\n\nIncidence Ratios \nReturning to the incidence rate from above, the Poisson model estimates the log of the expected counts of an event, given the predictor variables. We can get the expected counts of a given even by exponentiating both sides of the equation, so that\n \\mu = exp(\\alpha + \\beta X) \nOnce you have the incidence rate calculated, you can find the incidence ratio, which will tell you how the count value will change with a one-unit increase in the given independent variable. You can take the incidence rate to calculate the percent change in the response by doing the following:\n (Incidence Rate - 1) * 100 \n\n\nRunning it in R \nYou can run a Poisson regression in R by using either the glm function from the stats package or the vglm function from the VGAM package. The easiest way of doing this, though, is by using the glm function since that comes with base R. Because of that, I’ll be focusing on the glm function, but the code should not be too different for the vglm function.\n\npoi.mod &lt;- glm(y ~ x1 + x2 + x3, family = poisson, data = data)\nsummary(poi.mod)\n\npoi.irr &lt;- exp(coef(poi.mod))*sqrt(diag(vcov(poi.mod)))\n\n\n\nDiagnostic Statistics \nThe diagnostic statistics are mostly the same as those for other models covered, so this section will be brief. The one thing worth pointing out is that you can run a likelihood ratio test by fitting a null model.\n\nanova(poi.mod, update(poi.mod, ~ 1), test = \"Chisq\")\n\nThe above code will compare the fitted model to a null model using a Chi-squared test. A significant result indicates that the fitted model fits your data better than the null model.\nYou can also run Pseudo-R^2, AIC, and BIC tests for this model."
  },
  {
    "objectID": "posts/latex-citation-bibdata-bibstyle-issue.html",
    "href": "posts/latex-citation-bibdata-bibstyle-issue.html",
    "title": "LaTeX Issue Fix: No , , or commands",
    "section": "",
    "text": "Introduction\nThe Solution\nConclusion"
  },
  {
    "objectID": "posts/latex-citation-bibdata-bibstyle-issue.html#footnotes",
    "href": "posts/latex-citation-bibdata-bibstyle-issue.html#footnotes",
    "title": "LaTeX Issue Fix: No , , or commands",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you’d like to see the other proposed solutions, see this Stack Exchange thread that suggested a fresh Biber install and this one that suggested a fresh compile process. Neither solution was particularly helpful in my case because the issue persisted after I tried them.↩︎\nPlease don’t judge my package load order too harshly! I’m sure there’s something I could do to make it more efficient or potentially less problematic, but I haven’t made the time to do so at the time of writing.↩︎"
  },
  {
    "objectID": "posts/auto-emails-in-r.html",
    "href": "posts/auto-emails-in-r.html",
    "title": "Recurring Emails Using R",
    "section": "",
    "text": "Introduction\nSetting Up Gmailr\nGmailr Code\nAutomating with CronR"
  },
  {
    "objectID": "posts/auto-emails-in-r.html#footnotes",
    "href": "posts/auto-emails-in-r.html#footnotes",
    "title": "Recurring Emails Using R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that I saved my personal email as an RDS file so that I could call it in the script using readRDS() for security and ease. That way, I wouldn’t accidentally push my email to GitHub by hard-coding it into my script. However, running the code in this post can be just as easily done by setting the email address as a string, assuming it’ll just live on your local machine.↩︎\nIf you run this code and end up with any issues, make sure to look through the official documentation for the gmailr package. I had a few issues with the authorization when I initially ran this code, but I was able to resolve them by looking through the comprehensive documentation.↩︎"
  },
  {
    "objectID": "posts/phd/spring-2025-kickoff.html",
    "href": "posts/phd/spring-2025-kickoff.html",
    "title": "The Spring 2025 Semester Kicks Off",
    "section": "",
    "text": "This week marks the official start of the Spring 2025 semester, which for me will hopefully be the semester in which I become ABD1. I spent a good chunk of this week working on the slide deck for that defense and I’m feeling pretty good about it. However, the most notable thing that happened this week was the number of students who came by my office to ask for help with their work. I should note that this is not some random occurrence, but rather the result of my position in the department as the methods TA–the designated person to help graduate and undergraduate students with any research and methods-related questions they might have.\nPart of the attendance I got came about due to my class introductions where I dropped by most methods-related courses to introduce myself and let students know that I’m available to help them with their work. Most of the students I got to help were graduate students, though, most of whom came by to pick up on conversations we had had last semester. It makes me happy to see students coming by to utilize the resources available to them, and I’m glad attendance has already picked up faster than it did last semester.\nOne of the greatest pleasures I get from helping these students comes from how their questions make me think about my own work and what I’ve learned. I find that it’s easy to take certain bits of information for granted, whether that be due to the implicit assumptions one makes or the fact that it’s near-impossible to adequately question every element of a given topic. However, when I get these students in my office, I find that their questions often force me to think about statistics or programming in a different way or to fundamentally questions why I do something in some way–even if I know that way is correct.\nA good example of this came yesterday when I had a student ask me about some code I had helped him write. It was a chunk of R code that took a data frame, did some minor cleaning operations, generated some descriptive statistics, and ultimately displayed the result as a kable in R. For tasks such as that, I’ll usually begin with the data frame object and pipe through the requisite steps before displaying the table, with a major reason for that being my desire to avoid intermediate object assignment. However, the student asked me what the %&gt;% operator was and how it worked. Now, ignoring any debates one might encounter from R programmers or the tidyverse, I’ve often found that students who are new to R or programming in general have a hard time with the pipe operator–at least if the instructor didn’t begin with it when teaching the language. And, thinking back to my experience learning R, I can’t say I was any different; I remember more or less understanding what the pipe did without really having the intuition to use it in my own code.\nSo, I took a moment to explain the pipe operator to the student, showing him how you can use it to pass the object as an argument to subsequent functions. I also showed him the alternative to the pipe operator, which involves plenty of intermediate assignment and, in my view, makes the code harder to read. I think he understood what I was talking about, but the experience made me reflect on how I should model programming practices to students that come by. I don’t think it’s as helpful to help students write code (say, for a descriptive statistics table or data cleaning operation) if I’m going to do so by adding to the conceptual load they’re already carrying. For instance, if students are already struggling with cleaning a variable in their data frame, the goal here is that they learn how to do so, not necessarily to teach them the way that I do it. By including the pipe operator, or any other element of the tidyverse, for that matter, I’m making their lives harder if their instructor didn’t begin with the intention to teach them this side of R programming.\nOne of the most repeated lessons I’ve gotten when going through teaching training had to do with the importance of scaffolding in the classroom. In other words, an effective pedagogical approach begins with first principles and iteratively builds upon them until the student can be expected to grasp the more complex concept that makes up the course’s end goal. If, then, the students I’m encountering aren’t exposed to the tidyverse view of the world–which I have on good authority is the case–then I should be careful not to complicate matters by introducing them to something to which they’ve had no exposure. The alternative is to force them to learn something that they don’t need to know at the moment, which can frustrate them and ultimately lead to a greater attrition rate in the course.\nAs a tutor, I have an obligation to help students learn the material in an effective manner. This means that I should avoid laziness in my approach; I should make sure that I’m not just giving them the answer that is the most readily available to me or the easiest to explain. I should consider what they know, to what point they’ve been scaffolded, and what they need to know to get to the next level. With that information in mind, I can then help them learn the material in a more effective manner, thereby minimizing the frustration they might feel and increasing the likelihood that they’ll come back for more help in the future. In the end, what’s the use of a tutor who gives the right answer if the student doesn’t understand it or can’t replicate it on their own?"
  },
  {
    "objectID": "posts/phd/spring-2025-kickoff.html#footnotes",
    "href": "posts/phd/spring-2025-kickoff.html#footnotes",
    "title": "The Spring 2025 Semester Kicks Off",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nABD stands for “All But Dissertation.” It’s a term used to describe a PhD candidate who has completed all of the requirements for their degree except for the dissertation. In my program, this means that I’ll have defended my prospectus, passed my two comprehensive exams, and completed all of my degree’s necessary coursework.↩︎"
  },
  {
    "objectID": "posts/r-project-template.html",
    "href": "posts/r-project-template.html",
    "title": "How to Make an R Project Template",
    "section": "",
    "text": "Introduction\nSetup\nProject Function\nSetting up the Metadata"
  },
  {
    "objectID": "posts/r-project-template.html#footnotes",
    "href": "posts/r-project-template.html#footnotes",
    "title": "How to Make an R Project Template",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that the .dcf file is a simple text file, so you can create it in any text editor you prefer. I recommend using VSCode to generate it as it is relatively straightforward to do so there, but you can edit it using RStudio’s built-in text editor if you prefer.↩︎"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "This browser does not support PDFs. Please download the PDF to view it: &lt;a href='/static/public_cv.pdf'&gt;Download CV&lt;/a&gt;."
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Talks and Presentations",
    "section": "",
    "text": "On Troubleshooting\nUNLV Political Science | 15 October 2024\nA talk given to an incoming class of graduate students on the troubleshooting process in programming.\n\n\nView Full Screen\n\n\n¿Reputación Importa?\nUNLV Spanish Class | 10 Dec 2024\nA presentation given to my graduate-level Spanish class where I translated an early draft of a research project.\n\n\nView Full Screen"
  },
  {
    "objectID": "posts/phd/finished-prospectus.html",
    "href": "posts/phd/finished-prospectus.html",
    "title": "I Just Finished My Prospectus",
    "section": "",
    "text": "A part of me wants to start writing more things that aren’t academic. I’d like to write about what’s on my mind or about things that I find interesting–even if nobody is reading–but my degree progression consumers so much time that I rarely get the chance to do so. Starting this year, I’d like to make an effort to change that. Like I said, even if nobody is reading what I’m writing. I think it would be fun for posterity, and engaging in this type of writing might help me think more clearly about what I’m doing in the aggregate, which can be challenging when pushing through everything, one day at a time.\nTo that end, I think a good starting point is to try to write somewhat regularly about my progress through the PhD program–maybe about what I feel I’ve accomplished or learned in a given week. Now, I’m well aware that I said I want to write things that aren’t academic, and that what I’m proposing here is to write about the part of my life that is academic. But come on, I’ve got to start somewhere, and I think that writing sort of informally about what’s going on in this part of my life is distant enough from the academic writing I’m doing for my degree that I can scratch that writing itch I have.\nWith that house keeping out of the way, I just finished my prospectus yesterday. I felt pretty good about hitting that milestone. I’m well aware that this is only one part of a two-step process that is a prospectus defense, so I know I’m not out of the woods yet. Nevertheless, it felt good to get that document written and out of the way and sent to my committee. I’ve also gotten responses from all of my committee members regarding the defense date; it’s looking like we’ll be meeting at the end of January. The exact time is uncertain at this point–I still have to hear back from one of my committee members’ administrative assistant about his schedule–but I’m eager for my work to make it through that first crucible. I know my four committee member won’t go easy on me, and I’m confident their comments will be biting, but incredibly insightful and constructive.\nHaving finished my prospectus, I’m struck by a feeling that has been relatively common throughout my degree progression. There’s this almost necessary level of solitude that pervades most aspects of my degree progression. I’m not saying that there aren’t people around to talk with about my work or that I don’t have a support system–I’d say quite the opposite is true, in fact. No, there is a solitude that comes in the moment of rejoicing in the completion of a task. I can tell my friends and family that I did this thing, and I can talk to any number of people about everything that went into what I’ve done. And they’ll all express their congratulations or their well wishes, which is nice. But I can’t escape this feeling of solitude that comes about following a task’s completion.\nNow, I’m not saying that nobody understands what I’m doing or that nobody can relate. Everyone in my life understands and they all relate to most, if not all, I’m going through here. This is especially the case with my friends in the program, who are quite literally undergoing the same experience. After all, undertaking a PhD is not so substantively different from other types of work that it’s impossible or even difficult to relate to.\nI think the feeling of solitude comes from how much of myself I’ve put into my research, how much of my time and how many of my thoughts I’ve dedicated to this endeavor so far. And this goes for the prospectus, of course, but also for the broader undertaking as a whole, including my coursework, research, conference travel, and anything else that can be reasonably included under my degree progression umbrella.\nI spend most of my time thinking about this project, the data I’ll keep collecting, and helpful code I can write to make the process more efficient. I’ll think about potential article ideas and about moves I can make to add another line to my CV. I think I’ve trapped myself, thinking constantly in terms of me and what I can and should do next. When it comes to rejoicing in a tangible accomplishment, I can’t help but default to the same mindset: solitude resulting from the single-minded need to succeed.\nDespite the solitude, I don’t necessarily see a message of sadness in this. What I said above may be the case, but I nevertheless feel excited for what will come next. It does bring me joy to share my accomplishment with my friends and family, and I am earnestly looking forward to what my committee will say about where my research is heading. I also feel this underlying sense that I’m exactly where I’ve always wanted to be doing precisely what I’ve always wanted to do–even if I never could have imagine the granular details of what I’m doing now. Not only am I happy in the moment, I am content with the aggregate of my experiences so far, which I think is far more challenging to achieve.\nNow that this prospectus is done, I have plenty to be happy about, plenty to be thankful for. I am motivated to keep pushing. And, importantly, I’m looking forward to my defense, which will be the next step in this long journey. I hope I get to write about a successful defense soon."
  },
  {
    "objectID": "posts/tex-to-docx.html",
    "href": "posts/tex-to-docx.html",
    "title": "How to Convert a LaTeX Document to a DOCX File",
    "section": "",
    "text": "Introduction\nSetup\nConversion\nA Note on Complex Tables*\nConclusion"
  },
  {
    "objectID": "posts/tex-to-docx.html#footnotes",
    "href": "posts/tex-to-docx.html#footnotes",
    "title": "How to Convert a LaTeX Document to a DOCX File",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRegardless, I’d still prefer to have the table within the .tex document itself, rather than on its own. However, if you’re in the position of needing to convert a LaTeX document to a Word document, this is a solid workaround that gets the job done.↩︎"
  },
  {
    "objectID": "posts/horology/writing-on-watches.html",
    "href": "posts/horology/writing-on-watches.html",
    "title": "Writing on Watches",
    "section": "",
    "text": "I resolved at the turn of the year to write more, which I did for all of three or four weeks before I began to falter. From the outset, I settled on writing about my academic life–which I’m still interested in doing–because I thought it would be a good way to continue having material about which I could write on a weeklyish basis. When I set out to do this, I couldn’t think of much else to write about, though to be fair, I didn’t think too hard about what I could write.\nNow that my prospectus defense is done and I’m ABD (finally), I’ve been trying to think of other things I could write. The most obvious path is to keep writing about my academic life, perhaps expanding the topic to include things I’m learning about machine learning, international investment law, and other topics I’m pursuing. But I like other stuff too. It doesn’t all have to be about school.\nFortunately, I’ve been thinking a lot about watches recently. I’ve had a tiny collection for a while now, and I’ve been meaning to expand it, but other things keep getting in the way. So a good step to accomplish both goals–writing and expanding my watch collection–is to write about watches. Here I can find a topic that readily interests me while also developing a hobby. And goodness knows, I need more hobbies these days.\nI’m writing this inaugural post more to collect my thoughts than anything else. A game plan of sorts. If I’m going to write about watches, I’d like to have something of a plan I can loosely follow. As much as I love watches, I don’t think I could keep up a weekly schedule just on this, so I don’t want to start off with a lofty, albeit unrealistic, goal. As such, I won’t aim to write about watches on a schedule. Instead, I’ll write something here and there, whenever I buy a new watch or whenever I have something to say about a watch I already own.\nI think I’ll start with a post about the most recent watch I purchased, and 1948 Pobeda manufactured in the post-war Soviet Union. I’ve loved Russian watches for a while now and I’ve wanted to add some to my collection for as long as I can remember. This will be my first Russian watch, and, as I’ll probably elaborate on when I write about it after it arrives, its history is exciting and fascinating. I can’t wait for it to arrive.\nI’d also like to take some deep dives into the history of watchmaking, particularly in the Soviet Union. I’ve read a bit about it, and I’ve pieced together a rough timeline of the major events in Soviet watchmaking, but I’d like to delve deeper into the subject–especially as it relates to the pieces that I intend to buy.\nI would also like to learn some more about what makes a watch tick. I know some basics about mechanical movements, but I typically end up forgetting anything more technical I learn within a few days. I think here I’ll take a similar approach to what I’ve done to learn statistics and machine learning: whatever I want to learn I’ll break apart first, then put back together. I know there are a few watchmaking sets I can buy online that come with detailed instructions about putting together a simple movement. Another good option would be to just buy some cheap movements–maybe an NH35 or a Miyota–and take them apart. If I can successfully put them back together, then I’ll know I’ve learned something that’ll stick.\nI think that’ll be a good start. Broadly, I have three goals:\n\nWrite about the watches I own and the watches I buy.\nLearn more about the history of watchmaking, particularly in the Soviet Union, and write about it.\nLearn more about how watches work and write about my experiences learning.\n\nAll three of which I think will be fun and interesting for me to pursue. I’m not putting any pressure on myself to write on a schedule–at least not about watches–so I think I can keep this up more readily. I’ll try to write about other stuff too."
  },
  {
    "objectID": "posts/Notes/greek-letters.html",
    "href": "posts/Notes/greek-letters.html",
    "title": "Greek Alphabet in Markdown",
    "section": "",
    "text": "The following table is a recreation of Table 1.3 found in A Mathematics Course for Political & Social Research by Will H. Moore and David A. Siegel, with a few additions and changes. This table is a quick reference for Greek alphabet notation in Markdown and RMarkdown, and it can also be useful for formatting math equations in LaTeX.\nNote that the inline code snippets for the Greek letters’ lower case and upper case versions are the same, with the only difference relating to the capitalization of the letter in the inline code. For example, the inline code for the lower case alpha is $\\alpha$, and the inline code for the upper case alpha is $\\Alpha$.\n\n\n\nUpper case\nLower case\nName\nInline Code\n\n\n\n\n\\Alpha\n\\alpha\nAlpha\n$\\alpha$\n\n\n\\Beta\n\\beta\nBeta\n$\\beta$\n\n\n\\Gamma\n\\gamma\nGamma\n$\\Gamma$\n\n\n\\Delta\n\\delta\nDelta\n$\\Delta$\n\n\n\\Epsilon\n\\epsilon\nEpsilon\n$\\Epsilon$\n\n\n\\Zeta\n\\zeta\nZeta\n$\\Zeta$\n\n\n\\Eta\n\\eta\nEta\n$\\Eta$\n\n\n\\Theta\n\\theta\nTheta\n$\\Theta$\n\n\n\\Iota\n\\iota\nIota\n$\\Iota$\n\n\n\\Kappa\n\\kappa\nKappa\n$\\Kappa$\n\n\n\\Lambda\n\\lambda\nLambda\n$\\Lambda$\n\n\n\\Mu\n\\mu\nMu\n$\\Mu$\n\n\n\\Nu\n\\nu\nNu\n$\\Nu$\n\n\n\\Xi\n\\xi\nXi\n$\\Xi$\n\n\n\\Omicron\n\\omicron\nOmicron\n$\\Omicron$\n\n\n\\Pi\n\\pi\nPi\n$\\Pi$\n\n\n\\Rho\n\\rho\nRho\n$\\Rho$\n\n\n\\Sigma\n\\sigma\nSigma\n$\\Sigma$\n\n\n\\Tau\n\\tau\nTau\n$\\Tau$\n\n\n\\Upsilon\n\\upsilon\nUpsilon\n$\\Upsilon$\n\n\n\\Phi\n\\phi\nPhi\n$\\Phi$\n\n\n\\Chi\n\\chi\nChi\n$\\Chi$\n\n\n\\Psi\n\\psi\nPsi\n$\\Psi$\n\n\n\\Omega\n\\omega\nOmega\n$\\Omega$"
  },
  {
    "objectID": "posts/Notes/stereotype-logit.html",
    "href": "posts/Notes/stereotype-logit.html",
    "title": "Stereotype Logit Model",
    "section": "",
    "text": "I am writing quick and easy R guides for my didactic purposes and to provide useful starting places for my peers in grad school. If you see that I have made a mistake or would like to suggest some way to make the post better or more accurate, please feel free to email me. I am always happy to learn from others’ experiences!\n\nTable of contents\n\nModel Formula\nOdds Ratios\nRunning it in R\n\nNote that I explore much of the information for Sections 2-4 more thoroughly in other posts. Of particular utility will be my posts on logit models and continuation ratio models.\n\n\nModel Formula \nThe stereotype logit model is an interesting generalization of the proportional odds model, as we will see below because it adds a series of nuances to categorical data exploration. One of the primary use cases for this model instead of the PO model is when the proportional odds assumption does not hold, which is unsurprisingly common for most data. Of course, you could fit the PPO and GOL models if your data don’t follow the PO assumption, but the SL model might be more interesting.\nThe model’s equation is as follows:\n logit[\\pi (j, J)] = ln \\left( \\frac{P(Y = j | x_1, x_2, ..., x_p)}{P(Y = J | x_1, x_2, ..., x_p)} \\right) = \\alpha_j + \\phi_j(\\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p) \nwhere J is the baseline (i.e., last) category for this given equation, j=1,2,..., J - 1, Y is the ordinal response variable with J categories, _j represents the intercepts, the values are the coefficients for X variables, and _j represents the constraints or scale parameters used to determine whether the outcome variable is ordinal. Importantly, _j assumes that\n 1 = \\phi_1 &gt; \\phi_2 &gt; \\phi_3 &gt; ... &gt; \\phi_{J-1} &gt; \\phi_J = 0 \nNotice that the constraints are ordered from one to zero, with each subsequent constraint being smaller than the last. Therefore, \\phi_1 = 1 and \\phi_J = 0, the constraints in the middle falling somewhere in between. What this does is force the model to have ordered categorical variables. Suppose, then, that you have a variable where J = 4. Once you know what \\phi_1 and \\phi_4 are, the only constraints you have left to estimate are \\phi_2 and \\phi_3.\nAs you can probably guess from the model formula above, the values are distributed across your coefficients, leading to\n \\phi_1\\beta &gt; \\phi_2\\beta &gt; \\phi_3\\beta &gt; \\phi_4\\beta \nassuming we continue with our earlier example. As you can see here, the scale parameters (aka, constraints) ensure ordinality in the model.\nNaturally, this leads to a minor complication when calculating the odds of being in category j versus category m. Rather than simply exponentiating the coefficients, we take the exponential of [(\\alpha_j - \\alpha_m) + (\\phi_j - \\phi_m)\\beta]\n\n\nOdds Ratios \nThe stereotype logit model estimates the log odds of being in a given category relative to the baseline. Therefore, generating the odds ratio can be done by\n Odds(Y = j vs. Y = J) = \\frac{P(Y = j)}{P(Y = J)} \nwhere j can be any category from 1 to J - 1. Recall, however, that the log odds of being in a given category as opposed to the baseline need to be multiplied by the scale parameters 4_j$, as we saw above.\n\n\nRunning it in R \nRunning the stereotype logit model in r works with the rrvglm function from the VGAM package. An SL model would look like the following:\nsl.model &lt;- rrvglm(y ~ x1 + x2 + x3, family = multinomial, rank = 1, data = data)\nsummary(sl.model)\nwhere rank = 1 indicates to R that the model is a one-dimensional SL model.\nAfter you run your model, be sure to get the odds ratio table to interpret the results.\n\nsl.or &lt;- exp(coef(sl.model, matrix = TRUE))"
  },
  {
    "objectID": "posts/Notes/multinomial-logit.html",
    "href": "posts/Notes/multinomial-logit.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "I am writing quick and easy R guides for my didactic purposes and to provide useful starting places for my peers in grad school. If you see that I have made a mistake or would like to suggest some way to make the post better or more accurate, please feel free to email me. I am always happy to learn from others’ experiences!\n\nTable of contents\n\nModel Formula\nConditional Probabilities and Odds Ratios\nRunning it in R\nDiagnostic Statistics\n\n\n\nModel Formula \nThe multinomial logistic regression is used for nominal response variables with multiple unordered categories, like party ID variables. You can also use this for an ordinal response variable, assuming the proportional odds assumption doesn’t hold. This model differs from the proportional odds and partial proportional odds models because it compares the odds of being in a given category against the odds of being in the baseline category. Recall that the PO and PPO models compare the odds of being at or below a given category. The multinomial logit model can be expressed as\n ln \\left( \\frac{P(Y = j | x_1, x_2, ..., x_p)}{P(Y = J | x_1, x_2, ..., x_p)} \\right) = \\alpha_j + \\beta_{j1}X_1 + \\beta_{j2}X_2 + ... + \\beta_{jp}X_p  \nwhere j = 1,2, ..., J - 1. J is the baseline category, which can be any category but is usually the highest. \\alpha_j represents the intercepts and the beta values represent the logit coefficients. This model is similar to the generalized ordinal logit model in that it estimates J - 1 logit coefficients for each independent variable.\nAs you can see, this model is essentially a series of logit models, where being in a given category is coded as 1 and being in the baseline is a 0.\nIt’s important to note that this model assumes the dependent variable follows the multinomial distribution, which is an extension of the binomial distribution. The binomial distribution can be expressed as\n P(Y = k) = \\binom{n}{k}p^k (1-p)^{n-k} \nwhere \\binom{n}{k} is the binomial coefficient, k is the number of successes (1), n is the number of trials, and p is the success probability when the outcome is 1. Therefore,\n \\binom{n}{k} = \\frac{n!}{(n-k)! k!} \nSince our model has a response variable with multiple categories, the probability function can be expressed as\n P(n_1, n_2, ..., n_j) = \\frac{n!}{(n_1!n_2!...n_j!)}p_{1}^{n_1}p_{2}^{n_2}...p_{j}^{n_j} \nwhere j is the number of categories in the response, n_j is the number of observations for a given category, and p_j is the probability of choosing a given category.\nThe log-likelihood function for the multinomial distribution can be expressed as\n l(p, n) = \\sum^j_{i=1} n_i ln p_i + ln\\frac{n!}{n_1!n_2!...n_j!} \nwhere ln\\frac{n!}{n_1!n_2!...n_j!} is a constant term and does not involve parameter p. \\sum{j}{i=1} n_i ln p_i is the sum of the number of observations for a given category and the log probability of choosing that category.\n\n\nConditional Probabilities and Odds Ratios \nIf you want to calculate the odds of being in a category as opposed to the baseline, you can express the equation as\n Odds(Y = j vs. Y = J) = \\frac{P(Y = j)}{P(Y = J)} \nThe odds ratios for this model function almost exactly like those for the logit model.\n\n\nRunning it in R \nThere are three packages you can use to run the multinomial logit model: VGAM, nnet, and mlogit. Using VGAM, you can run the vglm function. nnet uses the multinom function. And mlogit uses the mlogit function. The syntax for the vglm function is\n\nmlm.mod &lt;- vglm(dv ~ iv1 + iv2 + ivp, multinomial(refLevel = 1), data = data)\nsummary(mlm.mod)\n\nwhere multinomial is the family function and the refLevel argument indicates which level will serve as the baseline.\nAfter running the code above, make sure to generate an odds ratio table, which can be done by running the following code:\n\nmlm.or &lt;- cbind(exp(coef(mlm.mod)), exp(confint(mlm.mod)))\n\n\n\nDiagnostic Statistics \nThe diagnostic statistics for this model are exactly the same as those for the logit model."
  },
  {
    "objectID": "posts/Notes/bayes-formula.html",
    "href": "posts/Notes/bayes-formula.html",
    "title": "Notes on Bayes’ Formula",
    "section": "",
    "text": "I am writing these quick notes for my didactic purposes and to provide useful starting places for my peers in grad school. If you see that I have made a mistake or would like to suggest some way to make the post better or more accurate, please feel free to email me. I am always happy to learn from others’ experiences!\n\nTable of contents\n\nBayes’ Theorem\nGoing Through an Example\n\n\n\nBayes’ Theorem \nBayes’ Theorem is a fundamental concept in probability theory that allows us to update our beliefs about the probability of an event occurring given new information. The formula is written as:\n P(A|B) = \\frac{P(B|A) * P(A)}{P(B)} \nwhere P(A|B) is the probability of event A occurring given that event B has occurred, P(B|A) is the probability of event B occurring given that event A has occurred, P(A) is the prior probability of event A occurring, and P(B) is the prior probability of event B occurring.\nThe formula can be interpreted as follows:\n\nP(A|B) is the posterior probability of event A occurring given that event B has occurred.\nP(B|A) is the likelihood of observing event B given that event A has occurred.\nP(A) is the prior probability of event A occurring.\nP(B) is the prior probability of event B occurring.\n\nP(B) can be calculated as:\n P(B) = P(B|A) * P(A) + P(B|\\neg A) * P(\\neg A) \nwhere P(\\neg A) is the probability of event A not occurring.\n\n\nGoing Through an Example \nLet’s go through an example to illustrate how Bayes’ Theorem can be used to update our beliefs about the probability of an event occurring given new information.\nSuppose you have two servers, Server 1 and Server 2. Server 1 hosts 70% of the website traffic and Server 2 hosts the remaining 30%. Server 1 has a 98% uptime rate, while Server 2 has 96% uptime rate. A user reports that the website is down. What is the probability that the website is down because of Server 2?\nLet’s define the events as follows:\n\nA: Server 1 hosts the website;\nB: Server 2 hosts the website;\nD: The website is down.\n\nWe want to calculate P(A|D), the probability that Server 1 hosts the website given that the website is down.\nWe can calculate P(A|R) using Bayes’ Theorem as follows:\n P(A|D) = \\frac{P(D|A) * P(A)}{P(D)} \nwhere P(D|A) is the probability of the website being down given that Server 1 was hosting it, P(A) is the prior probability of Server 1 hosting the site, and P(D) is the prior probability of the site being down.\nWe can calculate P(D|A) as the probability of the website being down given it’s hosted by Server 1 .02, and P(A) as the prior probability of the website being hosted by Server 1, which is .7.\nTo calculate P(D), we can use the law of total probability:\n P(D) = P(D|A) * P(A) + P(D|B) * P(B) \nwhere P(D|A) is the probability of the website being down given that it was hosted on Server 1, which is .02. P(A) is the prior probability that the website was hosted on Server 1, which is .7. P(D|B) is the probability of the website being down given that it was hosted on Server 2, which is .04, and P(B) is the prior probability of the website being on Server 2, which is .3.\nSubstituting the values into the formula, we get:\n P(D) = .7 * .02 + .3 * .04 = .026 \nNow we can substitute the values into the formula for P(A|R):\n P(A|R) = \\frac{.02 * .7}{.026} \\approx 0.5384615385 \nTherefore, the probability that the website is down because of Server 1 is approximately .54."
  },
  {
    "objectID": "posts/Notes/zero-inflated.html",
    "href": "posts/Notes/zero-inflated.html",
    "title": "Zero-Inflated Models",
    "section": "",
    "text": "I am writing quick and easy R guides for my didactic purposes and to provide useful starting places for my peers in grad school. If you see that I have made a mistake or would like to suggest some way to make the post better or more accurate, please feel free to email me. I am always happy to learn from others’ experiences!\n\nTable of contents\n\nModel Formula\nRunning it in R\nDiagnostic Statistics\n\n\n\nModel Formula \nWhen we are dealing with count data, the first step is always going to be to fit a Poisson or negative binomial model. However, sometimes this is not always the best way of handling count data because neither model handles zero inflation adequately. Zero inflation is bound to be an issue with many data for the Poisson model because, as one can probably imagine, this will likely lead to a significant difference between the response variable’s variance and mean (i.e., dispersion). The negative binomial model is not immune to such an issue either, so a zero-inflated model necessarily becomes more appropriate for our purposes.\nNow, just because the Poisson and negative binomial models are no longer appropriate for a response with zero inflation does not mean we forget about those models altogether. Just the opposite, in fact. The Poisson and negative binomial models play a key role in the zero-inflated models I’ll discuss below.\nBefore looking at those models, though, it is crucial to begin with a base understanding of what zero inflation means. Simply having zero values in count data does not mean there is zero inflation. Indeed, we likely expect there to be zero counts in a given response variable. For zero inflation to occur, we have to have a dependent variable with an excessive degree of zeroes.\nWhen fitting a zero-inflated model, we see that there are two steps to predicting the dependent variable: (1) the zero prediction and (2) the count prediction. In other words, the zero-inflated model begins by running a logistic regression, which, if we recall, can be expressed as\n logit(p) = \\alpha + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p \nThis half of the model will predict whether the outcome will be zero or a positive count integer(coded as 1 for zero predictions and 0 for count predictions). From this, we get the probability of observing a zero in the response. If the given predictors can significantly predict the probability of observing a zero, then we have a pretty good indication that there is zero inflation. Note that the model does not go on to exclude the observed zero values from the count half of the model.\nAfter the model accounts for the probability of observing zero values, we move on to the next step, which is the count prediction. Here, the model applies either a Poisson regression or negative binomial regression to the response variable, which is how we distinguish between a ZIP and ZINB model. If we recall, the Poisson model can be expressed as\n ln(\\mu) = \\alpha + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p \nand the negative binomial model can be expressed as\n ln(\\mu) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_pX_p + \\epsilon \nWe can therefore express the zero-inflated Poisson model as\n\n\\begin{aligned}\nPr(Y = 0) = \\pi + (1 - \\pi)e^{-\\lambda} \\\\\nPr(Y = y_i) = (1 - \\pi)\\frac{\\lambda^{y_1}e^{-\\lambda}}{y_{i}!} \\\\\ny_i = 1,2,3,...\n\\end{aligned}\n\nwhere y_i represents the non-negative count values in the response, \\lambda is the expected count for the ith observation in the Poisson model, and \\pi is the probability of extra zeros.\n\n\nRunning it in R \nRunning either type of zero-inflated model in r can be done using the zeroinfl function from the pscl package. Notice that the syntax for this model differs from other models insofar as this model asks us to specify linear predictors for both the zero and count sides. Therefore, we would write\n\nzip.mod &lt;- zeroinfl(dv ~ iv1 + iv2 + iv3 | iv4 + iv5 + iv6, data = data)\nsummary(zip.mod)\n\nNotice that the default is set to the zero-inflated Poisson model, but you can change that by adding the argument dist = \"negbin\" to specify a zero-inflated negative binomial model.\n\nzinb.mod &lt;- zeroinfl(dv ~ iv1 + iv2 + iv3 | iv4 + iv5 + iv6, dist = \"negbin\", data = data)\nsummary(zinb.mod)\n\nAs you can see from the two models specified above, there are different predictors for the zero half of the model than there are for the count side of the model. That does not necessarily need to be the case, of course. You can fit the same predictors to both sides if that is what your theory stipulates.\nImportantly, we can use\nzip.irr &lt;- cbind(exp(coef(zip.mod)), exp(confint(zip.mod)))\nto create an incidence rate ratio for ease of interpretation.\nWe can also compute the model’s standard errors by running\nexp(coef(zip.mod)) * sqrt(diag(vcov(zip.mod)))\n\n\nDiagnostic Statistics \nThe diagnostic statistics for these two models will be mostly the same as those used for other count and categorical variable models. Importantly, though, we also want to employ the vuong function from pscl to run a Vuong test. This test will give us insight into the difference between a zero-inflated model and a Poisson or negative binomial model. A significant result indicates that we prefer one model over another."
  },
  {
    "objectID": "posts/Notes/adjacent-categories-model.html",
    "href": "posts/Notes/adjacent-categories-model.html",
    "title": "Adjacent Categories Models",
    "section": "",
    "text": "I am writing quick and easy R guides for my didactic purposes and to provide useful starting places for my peers in grad school. If you see that I have made a mistake or would like to suggest some way to make the post better or more accurate, please feel free to email me. I am always happy to learn from others’ experiences!\n\nTable of contents\n\nModel Formula\nConditional Probabilities and Odds Ratios\nRunning it in R\nDiagnostic Statistics\n\n\n\nModel Formula \nThe adjacent categories model is similar to the continuation ratio model and the series of proportional odds models, differing only slightly in how the equation is specified. As the name indicates, the AC model estimates the odds of being in category j+1 as opposed to being in the lower category j of the given ordinal response variable. Simply put, this model takes category pairs and compares the odds of being in category j instead of category j+1. The model is written as\n logit[P(Y = j + 1|x_1, x_2, ..., x_p)] = ln \\left( \\frac{P(Y = j + 1 | x_1, x_2, ..., x_p)}{P(Y + j | x_1, x_2, ..., x_p)} \\right) = \\alpha_j + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_3X_3 \nwhere j represents J-1 categories, _j gives us the intercepts, and the betas are logit coefficients.\n\n\nConditional Probabilities and Odds Ratios \nUnsurprisingly, calculating this model’s odds is very similar to the CR model and PO model. In this case, the equation is\n Odds(Y = j vs. Y = j - 1) = \\frac{P(Y = j)}{P(Y = j-1)} \nwhere j is any category above zero.\nAnd like with the other models mentioned above, the first thing you do after running the model is exponentiate the odds to get an odds ratio. Interpreting the odds ratio differs only insofar as this model is marginally distinct from the earlier models.\n\n\nRunning it in R \nYou can run the AC model in R using the vglm function from the VGAM package, just like for the earlier models. The only difference is that the family argument should be acat, such that the model looks like\n\nmodel &lt;- vglm(dv ~ iv, family = acat(parallel = TRUE, reverse = FALSE), data = data)\nsummary(model)\nwhere parallel = TRUE indicates that the model abides by the proportional odds assumption.\nAnd as with the earlier models, you can get an odds ratio by running\n\nac.or &lt;- cbind(exp(coef(model)), exp(confint(model)))\nprint(ac.or)\n\n\nDiagnostic Statistics \nFor a deeper dive into the model fit statistics, see the Logit Model Notes and the CR Model Notes."
  },
  {
    "objectID": "posts/Notes/probability_axioms.html",
    "href": "posts/Notes/probability_axioms.html",
    "title": "Notes on Probability Axioms",
    "section": "",
    "text": "These are some quick notes on probability axioms for future reference.\nLet \\Omega be the sample space, F be the event space, and P be the probability measure, with (\\Omega, F, P) being a probability space. The probability of some event P(E) is the probability of some event E occurring.\nBoiling this down to simpler terms, the sample space \\Omega is the set of all possible outcomes. So, if I am flipping a coin,\n \\Omega = \\{H, T\\} \nbecause heads and tails are the only possible outcomes. The event space F is all the possible sets of outcomes that can occur. Keeping with the coin flip example,\n F = \\{\\emptyset, \\{H\\}, \\{T\\}, \\{H, T\\}\\} \nwhere \\emptyset is the empty set, \\{H\\} is the event of flipping heads, \\{T\\} is the event of flipping tails, and \\{H, T\\} is the event of flipping either heads or tails.\n\n\nThe first axiom stipulates the non-negativity of the probability measure for events E in the event space F. That is,\n P(E) \\in \\R , P(E) \\geq 0 \\forall E \\in F \nIn simple terms, this axiom states that the probability of an event occurring is always a real number greater than or equal to zero for all events in the event space.\n\n\n\nThe second axiom states that the probability of the entire sample space \\Omega is equal to 1. That is,\n P(\\Omega) = 1 \nwhich, in simple terms, means that the probability of at least one event from the event space occurring is 1.\n\n\n\nThe third axiom states that the probability of the union of disjoint events is equal to the sum of the probabilities of the individual events. That is,\n P(\\bigcup_\\{i=1\\}^\\{\\infty\\} E_i) = \\sum_\\{i=1\\}^\\{\\infty\\} P(E_i) \nwhere E_i are disjoint events in the event space F. In simpler terms, this axiom states that the probability of the union of two or more events is equal to the sum of the probabilities of the individual events. And, by disjoint events, I mean that the events are mutually exclusive; that is, they don’t share any common elements."
  },
  {
    "objectID": "posts/Notes/probability_axioms.html#axiom-1-non-negativity",
    "href": "posts/Notes/probability_axioms.html#axiom-1-non-negativity",
    "title": "Notes on Probability Axioms",
    "section": "",
    "text": "The first axiom stipulates the non-negativity of the probability measure for events E in the event space F. That is,\n P(E) \\in \\R , P(E) \\geq 0 \\forall E \\in F \nIn simple terms, this axiom states that the probability of an event occurring is always a real number greater than or equal to zero for all events in the event space."
  },
  {
    "objectID": "posts/Notes/probability_axioms.html#axiom-2-unit-measure",
    "href": "posts/Notes/probability_axioms.html#axiom-2-unit-measure",
    "title": "Notes on Probability Axioms",
    "section": "",
    "text": "The second axiom states that the probability of the entire sample space \\Omega is equal to 1. That is,\n P(\\Omega) = 1 \nwhich, in simple terms, means that the probability of at least one event from the event space occurring is 1."
  },
  {
    "objectID": "posts/Notes/probability_axioms.html#axiom-3-countable-additivity",
    "href": "posts/Notes/probability_axioms.html#axiom-3-countable-additivity",
    "title": "Notes on Probability Axioms",
    "section": "",
    "text": "The third axiom states that the probability of the union of disjoint events is equal to the sum of the probabilities of the individual events. That is,\n P(\\bigcup_\\{i=1\\}^\\{\\infty\\} E_i) = \\sum_\\{i=1\\}^\\{\\infty\\} P(E_i) \nwhere E_i are disjoint events in the event space F. In simpler terms, this axiom states that the probability of the union of two or more events is equal to the sum of the probabilities of the individual events. And, by disjoint events, I mean that the events are mutually exclusive; that is, they don’t share any common elements."
  },
  {
    "objectID": "posts/bingo-in-r.html",
    "href": "posts/bingo-in-r.html",
    "title": "How to Make a Bingo Card in R",
    "section": "",
    "text": "Introduction\nCreating the Bingo Card\nMaking a Bingo Card Generator"
  },
  {
    "objectID": "posts/bingo-in-r.html#footnotes",
    "href": "posts/bingo-in-r.html#footnotes",
    "title": "How to Make a Bingo Card in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI did this instead of using the extand Shiny app provided by Jenny Bryan’s and Dean Attali’s bingo package because I wanted to have more granular control over the cards’ appearance.↩︎"
  },
  {
    "objectID": "posts/migrating-to-quarto.html",
    "href": "posts/migrating-to-quarto.html",
    "title": "Migrating to Quarto",
    "section": "",
    "text": "Introduction\nHow I Migrated to Quarto\nA Few Useful Features\nConcluding Thoughts"
  },
  {
    "objectID": "posts/migrating-to-quarto.html#footnotes",
    "href": "posts/migrating-to-quarto.html#footnotes",
    "title": "Migrating to Quarto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMy old website was hosted on Vercel, which I was only able to figure out with my friend, Luis’s, help.↩︎\nR and Python code, that is, not JavaScript. See the second point, about brackets in math equations.↩︎"
  },
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "CLAUDE.html#essential-commands",
    "href": "CLAUDE.html#essential-commands",
    "title": "",
    "section": "Essential Commands",
    "text": "Essential Commands\n\nDevelopment\n\nquarto preview - Start local development server with live reload\nquarto render - Build the entire site for production\nquarto render posts/filename.qmd - Render a specific post\n\n\n\nContent Management\n\nquarto create post posts/new-post.qmd - Create new post with template\nPosts are created in posts/ directory with subdirectories:\n\nposts/Notes/ - Academic research notes\nposts/phd/ - PhD journey posts\n\nposts/horology/ - Personal interests\n\n\n\n\nDeployment\n\nAutomatic deployment via GitHub Actions on push to main branch\nManual deployment: git push origin main (triggers GitHub Actions workflow)\nSite deploys to: https://kevinnavreteparra.github.io/kevin-quarto-blog/"
  },
  {
    "objectID": "CLAUDE.html#architecture-overview",
    "href": "CLAUDE.html#architecture-overview",
    "title": "",
    "section": "Architecture Overview",
    "text": "Architecture Overview\nThis is a Quarto-based academic blog with the following structure:\n\nCore Configuration\n\n_quarto.yml - Main site configuration (theme: slate + custom.scss, KaTeX math, code tools)\ncustom.scss - Custom SCSS variables and styling overrides\nstyles.css - Additional CSS rules\n\n\n\nContent Architecture\n\nHomepage (index.qmd) - Displays post listings with category filtering\nStatic Pages: about.qmd, posts.qmd, projects.qmd\nBlog Posts - All in posts/ directory, organized by topic in subdirectories\nAssets - Images and downloads in assets/ directory\n\n\n\nKey Features\n\nAcademic focus with mathematical content support (KaTeX)\nCategory-based post filtering and RSS feeds\nCode highlighting and folding with R/statistical analysis support\nResponsive design with dark/light mode toggle\nGoogle Analytics integration with cookie consent\n\n\n\nPost Structure\nAll posts use YAML frontmatter with:\n---\ntitle: \"Post Title\"\nauthor: \"Kevin Navarrete-Parra\"\ndate: \"YYYY-MM-DD\"\ndescription: \"Brief description\"\ncategories: [research-methods, statistics, data-science]\nformat: html\ntoc: true\n---\n\n\nDeployment Pipeline\n\nGitHub Actions workflow (.github/workflows/quarto-publish.yml)\nAutomatic R environment setup with renv support\nPublishes to GitHub Pages on main branch pushes\nUses freeze: auto for computational caching"
  },
  {
    "objectID": "CLAUDE.html#security-environment-variables",
    "href": "CLAUDE.html#security-environment-variables",
    "title": "",
    "section": "Security & Environment Variables",
    "text": "Security & Environment Variables\nCritical Security Setup: - Sensitive variables (Google Analytics, email) are stored as environment variables - GitHub Secrets required: GOOGLE_ANALYTICS_ID, CONTACT_EMAIL - Local development: Create .env file (never commit this) - Use .env.example as template for required variables\nEnvironment Variable Loading:\n# Local development (bash/zsh)\nexport $(cat .env | xargs)\nquarto preview\n\n# PowerShell\nGet-Content .env | ForEach-Object { if ($_ -match \"^([^=]+)=(.*)$\") { [Environment]::SetEnvironmentVariable($matches[1], $matches[2]) } }"
  },
  {
    "objectID": "CLAUDE.html#important-notes",
    "href": "CLAUDE.html#important-notes",
    "title": "",
    "section": "Important Notes",
    "text": "Important Notes\n\nThis is an academic blog migrated from Next.js/MDX to Quarto\nContent focuses on political science research, statistical methods, and PhD journey\nMathematical expressions use LaTeX syntax with KaTeX rendering\nR code blocks should use Quarto execution options (#| echo: true, etc.)\nThe site uses a custom domain (kevinparra.co) configured via CNAME file\nNever commit sensitive data - always use environment variables for secrets"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kevin Navarrete-Parra",
    "section": "",
    "text": "Welcome to my website, where I write about things that interest me. I love programming in R and Python, and I enjoy learning about anything having to do with data science. And I like writing about programming, data science, my experience in academia, and watches when I get the chance.\nA few recent topics of interest include:\n\nUsing large language models for natural language inference tasks;\nBuilding machine learning pipelines;\nWorking with Word2Vec models for document similarity.\n\n\n\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nMigrating to Quarto\n\n\n\nR\n\nQuarto\n\nCode\n\n\n\nI migrated my website to Quarto!\n\n\n\nKevin Navarrete-Parra\n\n\nAug 12, 2025\n\n\n\n\n\n\n\n\n\n\n\nWriting on Watches\n\n\n\nHorology\n\nWriting\n\n\n\nSetting out to write more and explore a hobby.\n\n\n\nKevin Navarrete-Parra\n\n\nMar 6, 2025\n\n\n\n\n\n\n\n\n\n\n\nNotes on Probability Axioms\n\n\n\nNotes\n\nProbability\n\n\n\n\n\n\n\nKevin Navarrete-Parra\n\n\nFeb 3, 2025\n\n\n\n\n\n\n\n\n\n\n\nHow to Convert a Quarto Reveal.js Deck to PDF\n\n\n\nQuarto\n\nBash\n\nPDF\n\n\n\nA simple guide to converting a Quarto Reveal.js deck to a PDF\n\n\n\nKevin Navarrete-Parra\n\n\nJan 27, 2025\n\n\n\n\n\n\n\n\n\n\n\nThe Spring 2025 Semester Kicks Off\n\n\n\nPhD\n\n\n\nThe Spring semester has officially begun and I’ve already had a bunch of students come by.\n\n\n\nKevin Navarrete-Parra\n\n\nJan 25, 2025\n\n\n\n\n\n\n\n\n\n\n\nI Just Finished My Prospectus\n\n\n\nPhD\n\n\n\nI finished my prospectus and I’m looking forward to my defense.\n\n\n\nKevin Navarrete-Parra\n\n\nJan 18, 2025\n\n\n\n\n\n\n\n\n\n\n\nLaTeX Issue Fix: No , , or commands\n\n\n\nLaTeX\n\nbibliography\n\ncitations\n\nbiblatex\n\ntroubleshooting\n\n\n\nA potential fix for an annoying LaTeX error\n\n\n\nKevin Navarrete-Parra\n\n\nJan 3, 2025\n\n\n\n\n\n\n\n\n\n\n\nHow to Convert a LaTeX Document to a DOCX File\n\n\n\nLaTeX\n\nWord\n\nCode\n\nPandoc\n\n\n\nA quick guide to converting a LaTeX document to a DOCX file.\n\n\n\nKevin Navarrete-Parra\n\n\nDec 16, 2024\n\n\n\n\n\n\n\n\n\n\n\nHow to Make a Bingo Card in R\n\n\n\nR\n\nBingo\n\nCode\n\n\n\nA quick guide to making a bingo card in R.\n\n\n\nKevin Navarrete-Parra\n\n\nNov 3, 2024\n\n\n\n\n\n\n\n\n\n\n\nNotes on Bayes’ Formula\n\n\n\nNotes\n\nBayes\n\nFormula\n\nData Science\n\n\n\nQuick notes on Bayes’ Formula and how to Calculate it.\n\n\n\nKevin Navarrete-Parra\n\n\nSep 21, 2024\n\n\n\n\n\n\n\n\n\n\n\nHow to Make an R Project Template\n\n\n\nR\n\nRStudio\n\nCode\n\nProjects\n\n\n\nA quick guide to setting up an R project template in RStudio.\n\n\n\nKevin Navarrete-Parra\n\n\nMay 28, 2024\n\n\n\n\n\n\n\n\n\n\n\nGreek Alphabet in Markdown\n\n\n\nMarkdown\n\nRMarkdown\n\nGreek Alphabet\n\n\n\nQuick notes regarding Greek alphabet notation in markdown.\n\n\n\nKevin Navarrete-Parra\n\n\nMay 19, 2024\n\n\n\n\n\n\n\n\n\n\n\nMarkdown Notation\n\n\n\nMarkdown\n\nRMarkdown\n\nMath Notation\n\n\n\nQuick notes regarding mathematical notation in markdown.\n\n\n\nKevin Navarrete-Parra\n\n\nMay 19, 2024\n\n\n\n\n\n\n\n\n\n\n\nRecurring Emails Using R\n\n\n\nR\n\nemail\n\nautomation\n\n\n\nSimple code for using R to send recurring emails\n\n\n\nKevin Navarrete-Parra\n\n\nJan 11, 2024\n\n\n\n\n\n\n\n\n\n\n\nHow to Migrate All R Packages to a New Computer\n\n\n\nR\n\nRStudio\n\nCode\n\nPackages\n\n\n\nEasy way to migrate user-installed packages.\n\n\n\nKevin Navarrete-Parra\n\n\nOct 21, 2023\n\n\n\n\n\n\n\n\n\n\n\nZero-Inflated Models\n\n\n\nNotes\n\nModel\n\nData Science\n\nZero-Inflation\n\nCount Data\n\n\n\nQuick notes for Zero-inflated models.\n\n\n\nKevin Navarrete-Parra\n\n\nOct 18, 2023\n\n\n\n\n\n\n\n\n\n\n\nNegative Binomial Model\n\n\n\nNotes\n\nModel\n\nData Science\n\nCount\n\n\n\nQuick notes for the negative binomial model.\n\n\n\nKevin Navarrete-Parra\n\n\nOct 15, 2023\n\n\n\n\n\n\n\n\n\n\n\nPoisson Model\n\n\n\nNotes\n\nModel\n\nData Science\n\nPoisson\n\n\n\nQuick notes for Poisson Model.\n\n\n\nKevin Navarrete-Parra\n\n\nOct 8, 2023\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\nNotes\n\nModel\n\nData Science\n\nMultinomial\n\nLogit\n\n\n\nQuick notes for Multinomial Logit Model\n\n\n\nKevin Navarrete-Parra\n\n\nOct 2, 2023\n\n\n\n\n\n\n\n\n\n\n\nStereotype Logit Model\n\n\n\nNotes\n\nStereotyped Logit\n\nModel\n\nData Science\n\n\n\nQuick notes for Stereotyped Logit Models.\n\n\n\nKevin Navarrete-Parra\n\n\nSep 24, 2023\n\n\n\n\n\n\n\n\n\n\n\nAdjacent Categories Models\n\n\n\nNotes\n\nAdjacent Categories\n\nModel\n\nData Science\n\n\n\nQuick notes for Adjacent Categories Models.\n\n\n\nKevin Navarrete-Parra\n\n\nSep 24, 2023\n\n\n\n\n\n\n\n\n\n\n\nContinuation Ratio Model\n\n\n\nNotes\n\nContinuation Ratio\n\nModel\n\nData Science\n\n\n\nQuick notes for continuation ratio model.\n\n\n\nKevin Navarrete-Parra\n\n\nSep 23, 2023\n\n\n\n\n\n\n\n\n\n\n\nGeneralized Ordinal Logistic Regression Models and Partial Proportional Odds Models\n\n\n\nNotes\n\nProportional Odds\n\nModel\n\nData Science\n\nOrdered Logit\n\n\n\nQuick notes for PPO and GOL models.\n\n\n\nKevin Navarrete-Parra\n\n\nSep 15, 2023\n\n\n\n\n\n\n\n\n\n\n\nProportional Odds Models\n\n\n\nNotes\n\nOrdered Logit\n\nModel\n\nData Science\n\nProportional Odds\n\n\n\nQuick notes for the proportional odds model.\n\n\n\nKevin Navarrete-Parra\n\n\nSep 13, 2023\n\n\n\n\n\n\n\n\n\n\n\nLogit Notes\n\n\n\nNotes\n\nLogit\n\nModel\n\nData Science\n\n\n\nQuick notes for logit model.\n\n\n\nKevin Navarrete-Parra\n\n\nSep 6, 2023\n\n\n\n\n\n\nNo matching items"
  }
]